{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# own modules\n",
    "import eda_methods as eda\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# warnings handler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import fbeta_score, accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "random_state=101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data & set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature dataframe\n",
    "df_importance = pd.read_csv('data/df_clean_engineered_all.csv')\n",
    "\n",
    "# split label\n",
    "y = df_importance['churn']\n",
    "\n",
    "# drop obvious drops\n",
    "df_importance = df_importance.drop(['churn','plz_3','abo_registrierung_min','nl_registrierung_min','ort'], axis = 1)\n",
    "\n",
    "# get dummies\n",
    "df_importance = pd.get_dummies(df_importance, columns = ['kanal', 'objekt_name', 'aboform_name', 'zahlung_rhythmus_name','zahlung_weg_name', 'plz_1', 'plz_2', 'land_iso_code', 'anrede','titel'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defined list of important features\n",
    "important_features_combined_dropping = ['zahlung_weg_name_Rechnung',\n",
    "                                        'zahlung_rhythmus_name_halbjährlich',\n",
    "                                        'rechnungsmonat',\n",
    "                                        'received_anzahl_6m',\n",
    "                                        'openedanzahl_6m',\n",
    "                                        'objekt_name_ZEIT Digital',\n",
    "                                        'nl_zeitbrief',\n",
    "                                        'nl_aktivitaet',\n",
    "                                        'liefer_beginn_evt',\n",
    "                                        'cnt_umwandlungsstatus2_dkey',\n",
    "                                        'clickrate_3m',\n",
    "                                        'anrede_Frau',\n",
    "                                        'aboform_name_Geschenkabo',\n",
    "                                        'unsubscribed_anzahl_1m',\n",
    "                                        'studentenabo',\n",
    "                                        'received_anzahl_bestandskunden_6m',\n",
    "                                        'openrate_produktnews_3m',\n",
    "                                        'opened_anzahl_bestandskunden_6m',\n",
    "                                        'objekt_name_DIE ZEIT - CHRIST & WELT',\n",
    "                                        'nl_zeitshop',\n",
    "                                        'nl_opt_in_sum',\n",
    "                                        'nl_opened_1m',\n",
    "                                        'kanal_andere',\n",
    "                                        'kanal_B2B',\n",
    "                                        'clicked_anzahl_6m',\n",
    "                                        'che_reg',\n",
    "                                        'MONTH_DELTA_nl_min',\n",
    "                                        'zon_zp_red',\n",
    "                                        'zahlung_rhythmus_name_vierteljährlich',\n",
    "                                        'unsubscribed_anzahl_hamburg_1m',\n",
    "                                        'unsubscribed_anzahl_6m',\n",
    "                                        'sum_zon',\n",
    "                                        'sum_reg',\n",
    "                                        'shop_kauf',\n",
    "                                        'plz_2_10',\n",
    "                                        'plz_1_7',\n",
    "                                        'plz_1_1',\n",
    "                                        'openrate_zeitbrief_3m',\n",
    "                                        'openrate_produktnews_1m',\n",
    "                                        'openrate_3m',\n",
    "                                        'openrate_1m',\n",
    "                                        'nl_unsubscribed_6m',\n",
    "                                        'nl_fdz_organisch',\n",
    "                                        'metropole',\n",
    "                                        'cnt_abo_magazin',\n",
    "                                        'cnt_abo_diezeit_digital',\n",
    "                                        'cnt_abo',\n",
    "                                        'clicked_anzahl_bestandskunden_3m',\n",
    "                                        'aboform_name_Probeabo',\n",
    "                                        'aboform_name_Negative Option',\n",
    "                                        'MONTH_DELTA_abo_min']\n",
    "\n",
    "len(important_features_combined_dropping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184660, 307)\n",
      "(184660, 51)\n"
     ]
    }
   ],
   "source": [
    "# choose important features\n",
    "print(df_importance.shape)\n",
    "X = df_importance[important_features_combined_dropping]\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(modelname, y_train, y_test, predictions_train, predictions_test):\n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       -\n",
    "       - y_train: income training set\n",
    "       -\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    # model name\n",
    "    results['model'] = modelname\n",
    "    # accuracy\n",
    "    results['acc_train'] = accuracy_score(y_train,predictions_train)\n",
    "    results['acc_test'] = accuracy_score(y_test,predictions_test)\n",
    "    # F-score\n",
    "    #results[‘f_train’] = fbeta_score(y_train,predictions_train,0.5)\n",
    "    #results[‘f_test’] = fbeta_score(y_test,predictions_test,0.5)\n",
    "    # F1-score\n",
    "    results['f1_train'] = f1_score(y_train,predictions_train)\n",
    "    results['f1_test'] = f1_score(y_test,predictions_test)\n",
    "    # Recall\n",
    "    results['recall_train'] = recall_score(y_train,predictions_train)\n",
    "    results['recall_test'] = recall_score(y_test,predictions_test)\n",
    "    # Precision\n",
    "    results['precision_train'] = precision_score(y_train,predictions_train)\n",
    "    results['precision_test'] = precision_score(y_test,predictions_test)\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_optimization(X,y,balance=None):\n",
    "    \n",
    "    # devide features\n",
    "    categoric_features = list(X.columns[X.dtypes==object])\n",
    "    numeric_features = list(X.columns[X.dtypes != object])\n",
    "\n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state,stratify=y)\n",
    "    \n",
    "    if balance == 'over':\n",
    "        # define oversampling strategy\n",
    "        print('Oversampling')\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "    if balance == 'under':\n",
    "        print('Undersampling')\n",
    "        # define undersample strategy\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "        \n",
    "    # Hyperparameter grid\n",
    "    param_XGBoost = {\n",
    "        # tree param\n",
    "        'XGBoost__eta': [0.01, 0.2],                           # 0.001 to 0.2\n",
    "        'XGBoost__max_depth': [8, 9],                          # around 6 higher depth - overfitting\n",
    "        'XGBoost__min_child_weight': list(range(0,13,1)),      # min 0 no limit\n",
    "        'XGBoost__gamma': list(range(0, 5, 1)),                # the more conservative the algorithm will be\n",
    "        'XGBoost__subsample': list(range(0,9,2)),              # 3 out of 3 times 1       \n",
    "        # performance param\n",
    "        'XGBoost__sampling_method': ['uniform','gradient_based'],\n",
    "        'XGBoost__tree_method': ['approx','hist']              # 3 out of 4 times 'hist'\n",
    "    }\n",
    "        \n",
    "    models={\n",
    "        'XGBoost' : XGBClassifier(random_state=random_state, n_jobs=-1, n_estimators=100)\n",
    "        }  \n",
    "    \n",
    "    # create preprocessors\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer_num', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categoric_features)\n",
    "    ])\n",
    "\n",
    "    model_results = pd.DataFrame(columns=['model','acc_train','acc_test','f1_train','f1_test',\n",
    "                                          'recall_train','recall_test','precision_train','precision_test'])\n",
    "    \n",
    "    # process pipeline for every model\n",
    "    for model in models.items():\n",
    "        \n",
    "        print(model[0])\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                               (model[0], model[1])\n",
    "                               ])\n",
    "        \n",
    "        grid_XGBoost = RandomizedSearchCV(pipe, param_XGBoost, cv=5, scoring='recall', \n",
    "                           verbose=5, n_jobs=-1, n_iter = 100)\n",
    "        # fit model\n",
    "        grid_XGBoost.fit(X_train, y_train)\n",
    "        \n",
    "        # Show best parameters\n",
    "        print('Best score:{:.2f}'.format(grid_XGBoost.best_score_))\n",
    "        print('Best parameters:{}'.format(grid_XGBoost.best_params_))\n",
    "        \n",
    "        # Save best model as best_model\n",
    "        best_model = grid_XGBoost.best_estimator_\n",
    "        y_train_pred = grid_XGBoost.predict(X_train)\n",
    "        y_test_pred = grid_XGBoost.predict(X_test)\n",
    "        \n",
    "        results = train_predict(model[0],y_train, y_test, y_train_pred, y_test_pred)        \n",
    "        model_results = pd.concat([model_results, pd.DataFrame(results,index=[0])])\n",
    "\n",
    "        print(\"\\nConfusion matrix on test\")\n",
    "        print(confusion_matrix(y_test, y_test_pred))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampling\n",
      "XGBoost\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 30.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 33.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:02:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { validate_parameter } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Best score:\n",
      "0.71\n",
      "Best parameters:\n",
      "{'XGBoost__validate_parameter': False, 'XGBoost__tree_method': 'hist', 'XGBoost__subsample': 1, 'XGBoost__sampling_method': 'uniform', 'XGBoost__min_child_weight': 12, 'XGBoost__max_depth': 9, 'XGBoost__gamma': 5, 'XGBoost__eta': 0.1}\n",
      "\n",
      "Confusion matrix on test\n",
      "[[24407  7763]\n",
      " [ 3938 10057]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.79607</td>\n",
       "      <td>0.74654</td>\n",
       "      <td>0.792195</td>\n",
       "      <td>0.632218</td>\n",
       "      <td>0.777421</td>\n",
       "      <td>0.718614</td>\n",
       "      <td>0.807541</td>\n",
       "      <td>0.564366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model  acc_train  acc_test  f1_train   f1_test  recall_train  \\\n",
       "0  XGBoost    0.79607   0.74654  0.792195  0.632218      0.777421   \n",
       "\n",
       "   recall_test  precision_train  precision_test  \n",
       "0     0.718614         0.807541        0.564366  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomizedSearch_XGBoost = pipeline_optimization(X,y,balance='under')\n",
    "RandomizedSearch_XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second shot (higher fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampling\n",
      "XGBoost\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 29.2min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 41.7min\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed: 59.8min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 66.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:39:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { validate_parameter } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Best score:0.73\n",
      "Best parameters:{'XGBoost__validate_parameter': True, 'XGBoost__tree_method': 'approx', 'XGBoost__subsample': 1, 'XGBoost__sampling_method': 'gradient_based', 'XGBoost__min_child_weight': 4, 'XGBoost__max_depth': 9, 'XGBoost__gamma': 0, 'XGBoost__eta': 1}\n",
      "\n",
      "Confusion matrix on test\n",
      "[[23387  8783]\n",
      " [ 3657 10338]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.910313</td>\n",
       "      <td>0.730532</td>\n",
       "      <td>0.910366</td>\n",
       "      <td>0.624351</td>\n",
       "      <td>0.910897</td>\n",
       "      <td>0.738692</td>\n",
       "      <td>0.909835</td>\n",
       "      <td>0.540662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model  acc_train  acc_test  f1_train   f1_test  recall_train  \\\n",
       "0  XGBoost   0.910313  0.730532  0.910366  0.624351      0.910897   \n",
       "\n",
       "   recall_test  precision_train  precision_test  \n",
       "0     0.738692         0.909835        0.540662  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomizedSearch_XGBoost = pipeline_optimization(X,y,balance='under')\n",
    "RandomizedSearch_XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampling\n",
      "XGBoost\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   39.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   39.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:0.72\n",
      "Best parameters:{'XGBoost__tree_method': 'hist', 'XGBoost__subsample': 1, 'XGBoost__sampling_method': 'gradient_based', 'XGBoost__min_child_weight': 11, 'XGBoost__max_depth': 9, 'XGBoost__gamma': 2, 'XGBoost__eta': 0.2}\n",
      "\n",
      "Confusion matrix on test\n",
      "[[23834  8336]\n",
      " [ 3916 10079]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.816339</td>\n",
       "      <td>0.734604</td>\n",
       "      <td>0.814532</td>\n",
       "      <td>0.621969</td>\n",
       "      <td>0.806598</td>\n",
       "      <td>0.720186</td>\n",
       "      <td>0.822625</td>\n",
       "      <td>0.547326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model  acc_train  acc_test  f1_train   f1_test  recall_train  \\\n",
       "0  XGBoost   0.816339  0.734604  0.814532  0.621969      0.806598   \n",
       "\n",
       "   recall_test  precision_train  precision_test  \n",
       "0     0.720186         0.822625        0.547326  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomizedSearch_XGBoost = pipeline_optimization(X,y,balance='under')\n",
    "RandomizedSearch_XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "It was possible to improve the recall from min 0.718 to max 0.738 and the F1 score was pushed to 0.638 (worst: 0.624). So both scores we are targeting in this model are imprved. Let´s find the perfect parameter with a GridSearchCV and use the best param list to define it.\n",
    "\n",
    "- Best parameters:{'XGBoost__validate_parameter': False, 'XGBoost__tree_method': 'hist', 'XGBoost__subsample': 1, 'XGBoost__sampling_method': 'uniform', 'XGBoost__min_child_weight': 12, 'XGBoost__max_depth': 9, 'XGBoost__gamma': 5, 'XGBoost__eta': 0.1}\n",
    "\n",
    "- Best parameters:{'XGBoost__validate_parameter': True, 'XGBoost__tree_method': 'approx', 'XGBoost__subsample': 1, 'XGBoost__sampling_method': 'gradient_based', 'XGBoost__min_child_weight': 4, 'XGBoost__max_depth': 9, 'XGBoost__gamma': 0, 'XGBoost__eta': 1}\n",
    "\n",
    "- Best parameters:{'XGBoost__tree_method': 'hist', 'XGBoost__subsample': 1, 'XGBoost__sampling_method': 'gradient_based', 'XGBoost__min_child_weight': 11, 'XGBoost__max_depth': 9, 'XGBoost__gamma': 2, 'XGBoost__eta': 0.2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_optimization(X,y,balance=None):\n",
    "    \n",
    "    # devide features\n",
    "    categoric_features = list(X.columns[X.dtypes==object])\n",
    "    numeric_features = list(X.columns[X.dtypes != object])\n",
    "\n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state,stratify=y)\n",
    "    \n",
    "    if balance == 'over':\n",
    "        # define oversampling strategy\n",
    "        print('Oversampling')\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "    if balance == 'under':\n",
    "        print('Undersampling')\n",
    "        # define undersample strategy\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Hyperparameter grid\n",
    "    param_XGBoost = {\n",
    "        # tree param\n",
    "        'XGBoost__eta': [0.1,0.2],\n",
    "        'XGBoost__max_depth': [8,9],                           # around 6 higher depth - overfitting\n",
    "        'XGBoost__min_child_weight': list(range(4,10,2)),      # min 0 no limit\n",
    "        'XGBoost__gamma': list(range(0,3,1)),                  # the more conservative the algorithm will be\n",
    "        'XGBoost__subsample': [1],                             # 3 out of 3 times 1       \n",
    "        # performance param\n",
    "        'XGBoost__validate_parameter': [True],\n",
    "        'XGBoost__sampling_method': ['gradient_based'],\n",
    "        'XGBoost__tree_method': ['hist']                       # 3 out of 4 times 'hist'\n",
    "    }\n",
    "        \n",
    "    models={\n",
    "        'XGBoost' : XGBClassifier(random_state=random_state, n_jobs=-1, n_estimators=100)\n",
    "        }  \n",
    "    \n",
    "    # create preprocessors\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer_num', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categoric_features)\n",
    "    ])\n",
    "\n",
    "    model_results = pd.DataFrame(columns=['model','acc_train','acc_test','f1_train','f1_test',\n",
    "                                          'recall_train','recall_test','precision_train','precision_test'])\n",
    "    \n",
    "    # process pipeline for every model\n",
    "    for model in models.items():\n",
    "        \n",
    "        print(model[0])\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                               (model[0], model[1])\n",
    "                               ])\n",
    "        \n",
    "        grid_XGBoost = GridSearchCV(pipe, param_XGBoost, cv=5, scoring='recall', \n",
    "                           verbose=5, n_jobs=-1)\n",
    "        # fit model\n",
    "        grid_XGBoost.fit(X_train, y_train)\n",
    "        \n",
    "        # Show best parameters\n",
    "        print(\"\\n\")\n",
    "        print('Best score: {:.2f}'.format(grid_XGBoost.best_score_))\n",
    "        print('Best parameters: {}'.format(grid_XGBoost.best_params_))\n",
    "        \n",
    "        # Save best model as best_model\n",
    "        best_model = grid_XGBoost.best_estimator_\n",
    "        y_train_pred = grid_XGBoost.predict(X_train)\n",
    "        y_test_pred = grid_XGBoost.predict(X_test)\n",
    "        \n",
    "        results = train_predict(model[0],y_train, y_test, y_train_pred, y_test_pred)        \n",
    "        model_results = pd.concat([model_results, pd.DataFrame(results,index=[0])])\n",
    "\n",
    "        print(\"\\nConfusion matrix on test\")\n",
    "        print(confusion_matrix(y_test, y_test_pred))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampling\n",
      "XGBoost\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 27.9min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 39.8min\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed: 54.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed: 70.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1350 out of 1350 | elapsed: 83.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:41:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { validate_parameter } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best score: 0.73\n",
      "Best parameters: {'XGBoost__eta': 0.2, 'XGBoost__gamma': 0, 'XGBoost__max_depth': 9, 'XGBoost__min_child_weight': 4, 'XGBoost__sampling_method': 'gradient_based', 'XGBoost__subsample': 1, 'XGBoost__tree_method': 'hist', 'XGBoost__validate_parameter': True}\n",
      "\n",
      "Confusion matrix on test\n",
      "[[23984  8186]\n",
      " [ 3809 10186]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.844504</td>\n",
       "      <td>0.740171</td>\n",
       "      <td>0.843394</td>\n",
       "      <td>0.629406</td>\n",
       "      <td>0.837418</td>\n",
       "      <td>0.727831</td>\n",
       "      <td>0.849456</td>\n",
       "      <td>0.554431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model  acc_train  acc_test  f1_train   f1_test  recall_train  \\\n",
       "0  XGBoost   0.844504  0.740171  0.843394  0.629406      0.837418   \n",
       "\n",
       "   recall_test  precision_train  precision_test  \n",
       "0     0.727831         0.849456        0.554431  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearch_XGBoost = pipeline_optimization(X,y,balance='under')\n",
    "GridSearch_XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
