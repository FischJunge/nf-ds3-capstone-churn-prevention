{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize KNN\n",
    "This Notebook contains the optimized KNN model with the 51 selected features. This dataset contains multiple categorical variables and a few numerical variables. Since this is a supervised classification problem, we can apply a popular classification algorithm like KNN. The K-nearest neighbor method is the simplest classification method that classifies based on distance measures and based on our assumption,\n",
    "that given histories of other customers and the current customer’s data is a good fit to predict customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# own modules\n",
    "import eda_methods as eda\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# warnings handler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import fbeta_score, accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "random_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "def train_predict(modelname, y_train, y_test, predictions_train, predictions_test):\n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       -\n",
    "       - y_train: income training set\n",
    "       -\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    # model name\n",
    "    results['model'] = modelname\n",
    "    # accuracy\n",
    "    results['acc_train'] = accuracy_score(y_train,predictions_train)\n",
    "    results['acc_test'] = accuracy_score(y_test,predictions_test)\n",
    "    # F-score\n",
    "    #results[‘f_train’] = fbeta_score(y_train,predictions_train,0.5)\n",
    "    #results[‘f_test’] = fbeta_score(y_test,predictions_test,0.5)\n",
    "    # F1-score\n",
    "    results['f1_train'] = f1_score(y_train,predictions_train)\n",
    "    results['f1_test'] = f1_score(y_test,predictions_test)\n",
    "    # Recall\n",
    "    results['recall_train'] = recall_score(y_train,predictions_train)\n",
    "    results['recall_test'] = recall_score(y_test,predictions_test)\n",
    "    # Precision\n",
    "    results['precision_train'] = precision_score(y_train,predictions_train)\n",
    "    results['precision_test'] = precision_score(y_test,predictions_test)\n",
    "    #fbets\n",
    "    results['fbeta_train'] = fbeta_score(y_train, predictions_train, beta = .5, average = 'weighted').round(2)\n",
    "    results['fbeta_test'] =fbeta_score(y_test, predictions_test, beta = .5, average = 'weighted').round(2)\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature dataframe\n",
    "df = pd.read_csv('data/df_clean_engineered_all.csv')\n",
    "y = df['churn']\n",
    "# Drop some obvious unnecessary features\n",
    "df = df.drop(['churn','plz_3','abo_registrierung_min','nl_registrierung_min','ort'], axis = 1)\n",
    "# Get dummies for the categorial features\n",
    "df = pd.get_dummies(df, columns = ['kanal', 'objekt_name', 'aboform_name', 'zahlung_rhythmus_name',\n",
    "                                   'zahlung_weg_name', 'plz_1', 'plz_2', 'land_iso_code', \n",
    "                                   'anrede','titel'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184660, 307)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184660, 51)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 51 selected features \n",
    "X = df[['zahlung_weg_name_Rechnung',\n",
    " 'zahlung_rhythmus_name_halbjährlich',\n",
    " 'rechnungsmonat',\n",
    " 'received_anzahl_6m',\n",
    " 'openedanzahl_6m',\n",
    " 'objekt_name_ZEIT Digital',\n",
    " 'nl_zeitbrief',\n",
    " 'nl_aktivitaet',\n",
    " 'liefer_beginn_evt',\n",
    " 'cnt_umwandlungsstatus2_dkey',\n",
    " 'clickrate_3m',\n",
    " 'anrede_Frau',\n",
    " 'aboform_name_Geschenkabo',\n",
    " 'unsubscribed_anzahl_1m',\n",
    " 'studentenabo',\n",
    " 'received_anzahl_bestandskunden_6m',\n",
    " 'openrate_produktnews_3m',\n",
    " 'opened_anzahl_bestandskunden_6m',\n",
    " 'objekt_name_DIE ZEIT - CHRIST & WELT',\n",
    " 'nl_zeitshop',\n",
    " 'nl_opt_in_sum',\n",
    " 'nl_opened_1m',\n",
    " 'kanal_andere',\n",
    " 'kanal_B2B',\n",
    " 'clicked_anzahl_6m',\n",
    " 'che_reg',\n",
    " 'MONTH_DELTA_nl_min',\n",
    " 'zon_zp_red',\n",
    " 'zahlung_rhythmus_name_vierteljährlich',\n",
    " 'unsubscribed_anzahl_hamburg_1m',\n",
    " 'unsubscribed_anzahl_6m',\n",
    " 'sum_zon',\n",
    " 'sum_reg',\n",
    " 'shop_kauf',\n",
    " 'plz_2_10',\n",
    " 'plz_1_7',\n",
    " 'plz_1_1',\n",
    " 'openrate_zeitbrief_3m',\n",
    " 'openrate_produktnews_1m',\n",
    " 'openrate_3m',\n",
    " 'openrate_1m',\n",
    " 'nl_unsubscribed_6m',\n",
    " 'nl_fdz_organisch',\n",
    " 'metropole',\n",
    " 'cnt_abo_magazin',\n",
    " 'cnt_abo_diezeit_digital',\n",
    " 'cnt_abo',\n",
    " 'clicked_anzahl_bestandskunden_3m',\n",
    " 'aboform_name_Probeabo',\n",
    " 'aboform_name_Negative Option',\n",
    " 'MONTH_DELTA_abo_min']]\n",
    "        \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_optimization(X,y,balance=None):\n",
    "    \n",
    "    # devide features\n",
    "    categoric_features = list(X.columns[X.dtypes==object])\n",
    "\n",
    "    numeric_features = list(X.columns[X.dtypes != object])\n",
    "\n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state,stratify=y)\n",
    "    \n",
    "    if balance == 'over':\n",
    "        # define oversampling strategy\n",
    "        print('Oversampling')\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "    if balance == 'under':\n",
    "        print('Undersampling')\n",
    "        # define undersample strategy\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "        \n",
    "    # Hyperparameter grid\n",
    "    param_knn = {'KNN__n_neighbors':[10,15,20],\n",
    "                 'KNN__weights':['uniform', 'distance'],\n",
    "                 'KNN__p': [1,2],\n",
    "                 'KNN__metric':['euclidean', 'manhattan', 'minkowski']\n",
    "                }\n",
    "\n",
    "    \n",
    "    models={\n",
    "        'KNN' : KNeighborsClassifier(n_jobs=-1)\n",
    "        }  \n",
    "    \n",
    "    # create preprocessors\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_num', SimpleImputer(strategy='median')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categoric_features)\n",
    "        ])\n",
    "\n",
    "    model_results = pd.DataFrame(columns=['model','acc_train','acc_test','f1_train','f1_test','recall_train','recall_test','precision_train','precision_test'])\n",
    "    \n",
    "    \n",
    "    # process pipeline for every model\n",
    "    for model in models.items():\n",
    "        \n",
    "        print(model[0])\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                               (model[0], model[1])\n",
    "                               ])\n",
    "\n",
    "        grid_knn = RandomizedSearchCV(pipe, param_knn, cv=3, scoring='precision', \n",
    "                           verbose=5, n_jobs=-1, n_iter = 100)\n",
    "        # fit model\n",
    "        grid_knn.fit(X_train, y_train)\n",
    "    \n",
    "        # Show best parameters\n",
    "        print('Best score:\\n{:.2f}'.format(grid_knn.best_score_))\n",
    "        print(\"Best parameters:\\n{}\".format(grid_knn.best_params_))\n",
    "        \n",
    "        # Save best model as best_model\n",
    "        best_model = grid_knn.best_estimator_\n",
    "        \n",
    "        #predict results\n",
    "        y_train_pred = grid_knn.predict(X_train)\n",
    "\n",
    "        y_test_pred = grid_knn.predict(X_test)\n",
    "        \n",
    "        results = train_predict(model[0],y_train, y_test, y_train_pred, y_test_pred)\n",
    "        \n",
    "        model_results = pd.concat([model_results, pd.DataFrame(results,index=[0])])\n",
    "        # print results\n",
    "        print(\"\\nConfusion matrix on test\")\n",
    "        print(confusion_matrix(y_test, y_test_pred))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampling\n",
      "KNN\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 34.5min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 59.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.70\n",
      "Best parameters:\n",
      "{'KNN__weights': 'distance', 'KNN__p': 1, 'KNN__n_neighbors': 20, 'KNN__metric': 'manhattan'}\n",
      "\n",
      "Confusion matrix on test\n",
      "[[22084 10086]\n",
      " [ 3174 10821]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomizedSearch_knn = pipeline_optimization(X,y,balance='under')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>fbeta_train</th>\n",
       "      <th>fbeta_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>0.712769</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>0.620079</td>\n",
       "      <td>0.998833</td>\n",
       "      <td>0.773205</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>0.517578</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  acc_train  acc_test  f1_train   f1_test  recall_train  recall_test  \\\n",
       "0   KNN   0.999321  0.712769  0.999321  0.620079      0.998833     0.773205   \n",
       "\n",
       "   precision_train  precision_test  fbeta_train  fbeta_test  \n",
       "0         0.999809        0.517578          1.0        0.75  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomizedSearch_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The Random Grid Search resulted in a result of __recall: 0.77__, __f1: 0.62__ and a __precision: 0.52__. All three score are significant improvements to the first results of __recall: 0.46__, __f1: 0.51__ and a __precision: 0.58__ from the not tuend and unengineered baseline model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
