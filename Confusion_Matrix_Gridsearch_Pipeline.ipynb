{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nice Confusion Matrix Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = np.array([[24489,  7681],[ 3260, 10735]])\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                conf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     conf_matrix.flatten()/np.sum(conf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.set(font_scale=1.6)\n",
    "fig, ax = plt.subplots(figsize=(8,7)) \n",
    "sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='Blues',annot_kws={\"size\": 16})\n",
    "plt.title('Confusion Matrix: Random Forest (optimized)', fontsize = 16); # title with fontsize 20\n",
    "plt.xlabel('Predicted', fontsize = 16);\n",
    "plt.ylabel('Actual', fontsize = 16);\n",
    "#plt.savefig('plots/confusion_matrix_RF_optimized.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridearch Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_optimization_gridsearch(X,y,balance=None):\n",
    "    \n",
    "    # devide features\n",
    "    categoric_features = list(X.columns[X.dtypes==object])\n",
    "\n",
    "    numeric_features = list(X.columns[X.dtypes != object])\n",
    "\n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state,stratify=y)\n",
    "    \n",
    "    if balance == 'over':\n",
    "        # define oversampling strategy\n",
    "        print('Oversampling')\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "    if balance == 'under':\n",
    "        print('Undersampling')\n",
    "        # define undersample strategy\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "        \n",
    "    # Hyperparameter grid\n",
    "    param_randomforest = {\n",
    "    'randomforest__criterion' : ['gini', 'entropy'],\n",
    "    'randomforest__n_estimators': [360,380,400,420,440], #np.linspace(10, 500,50).astype(int),\n",
    "    'randomforest__max_depth': [32,35,37],\n",
    "    'randomforest__max_features': ['auto'],\n",
    "    'randomforest__bootstrap': [True],\n",
    "    'randomforest__min_samples_leaf': [1, 2],\n",
    "    'randomforest__min_samples_split': [3, 4, 5, 6, 7],\n",
    "    }\n",
    "    \n",
    "    models={\n",
    "        #'KNN' : KNeighborsClassifier(n_neighbors=5, metric='euclidean',n_jobs=-1),\n",
    "        #'SVC' : LinearSVC(),\n",
    "        #'logreg': LogisticRegression(random_state=random_state,n_jobs=-1), \n",
    "        #'decisiontree': DecisionTreeClassifier(random_state=random_state,max_depth=10),\n",
    "        'randomforest': RandomForestClassifier(random_state=random_state, n_jobs=-1,n_estimators=300)\n",
    "        #'XGBoost' : XGBClassifier(random_state=random_state, n_jobs=-1),\n",
    "        #'adaBoost' : AdaBoostClassifier(random_state=random_state)\n",
    "        }  \n",
    "    \n",
    "    # create preprocessors\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_num', SimpleImputer(strategy='median')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "        #('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categoric_features)\n",
    "        ])\n",
    "\n",
    "    model_results = pd.DataFrame(columns=['model','acc_train','acc_test','f1_train','f1_test','recall_train','recall_test','precision_train','precision_test'])\n",
    "    \n",
    "    # process pipeline for every model\n",
    "    for model in models.items():\n",
    "        \n",
    "        print(model[0])\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                               (model[0], model[1])\n",
    "                               ])\n",
    "        \n",
    "        grid_randomforest = GridSearchCV(pipe, param_grid = param_randomforest, \n",
    "                              cv = 3, scoring='f1', n_jobs = -1, verbose = 5)\n",
    "        # fit model\n",
    "        grid_randomforest.fit(X_train, y_train)\n",
    "        #pipe.fit(X_train, y_train)\n",
    "        \n",
    "        # Show best parameters\n",
    "        print('Best score:\\n{:.2f}'.format(grid_randomforest.best_score_))\n",
    "        print(\"Best parameters:\\n{}\".format(grid_randomforest.best_params_))\n",
    "        \n",
    "        # Save best model as best_model\n",
    "        best_model = grid_randomforest.best_estimator_\n",
    "        \n",
    "        #pipe_best = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "        #                       (model[0], best_model)\n",
    "        #                       ])\n",
    "        \n",
    "        # fit best pipe\n",
    "        #pipe_best.fit(X_train, y_train)\n",
    "        #predict results\n",
    "        #y_train_pred = cross_val_predict(grid_randomforest,X_train,y_train,cv=5)\n",
    "        y_train_pred = grid_randomforest.predict(X_train)\n",
    "        #y_train_pred = cross_val_predict(pipe_best, X_train, y_train, cv=5)\n",
    "\n",
    "        y_test_pred = grid_randomforest.predict(X_test)\n",
    "        \n",
    "        results = train_predict(model[0],y_train, y_test, y_train_pred, y_test_pred)\n",
    "        \n",
    "        model_results = pd.concat([model_results, pd.DataFrame(results,index=[0])])\n",
    "        # print results\n",
    "        #print(\"\\nResults on training data: \")\n",
    "        #print(classification_report(y_train, y_train_pred))\n",
    "        #print(\"\\nResults on test data:\")\n",
    "        #print(classification_report(y_test, y_test_pred))\n",
    "        print(\"\\nConfusion matrix on test\")\n",
    "        print(confusion_matrix(y_test, y_test_pred))\n",
    "        print(\"\\n\")\n",
    "    return model_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
